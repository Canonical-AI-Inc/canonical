{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7z3Ugy1R.dT5sLE2cbmAnTf41SybUEABmbninWIRZ\n",
      "http://localhost:8001/\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "print(os.environ[\"CANONICAL_CACHE_API_KEY\"])\n",
    "print(os.environ[\"CANONICAL_CACHE_HOST\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0\n",
    "CACHECLIENT = openai.OpenAI(\n",
    "    base_url=os.environ[\"CANONICAL_CACHE_HOST\"],\n",
    "    api_key=\"canbeanything\",\n",
    "    http_client=httpx.Client(\n",
    "        headers={\n",
    "            \"X-Canonical-Api-Key\": os.environ[\"CANONICAL_CACHE_API_KEY\"],\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "OPENAICLIENT = openai.OpenAI(\n",
    "    api_key= os.environ[\"OPENAI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(\n",
    "    client: openai.OpenAI,\n",
    "    messages: List[dict[str:str]],\n",
    "    stream: bool,\n",
    "    temperature: int = 0,\n",
    ") -> openai.ChatCompletion:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        stream=stream,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\n",
    "def displaycompletion(completion: openai.ChatCompletion, stream: bool) -> str:\n",
    "    msg = \"\"\n",
    "    if stream:\n",
    "        for chunk in completion:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                chunk = chunk.choices[0].delta.content\n",
    "                msg += chunk\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "    else:\n",
    "        msg = completion.choices[0].message.content\n",
    "        print(msg)\n",
    "    return msg\n",
    "\n",
    "\n",
    "def updatecache(messages: List[dict[str:str]], temp: int = 0) -> None:\n",
    "    requests.request(\n",
    "        method=\"POST\",\n",
    "        url=f\"{os.environ['CANONICAL_CACHE_HOST']}api/v1/cache\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Canonical-Api-Key\": os.environ[\"CANONICAL_CACHE_API_KEY\"],\n",
    "        },\n",
    "        data=json.dumps(\n",
    "            {\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": temp,\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "async def ask_canonical(message: str) -> None:\n",
    "    global chat_history\n",
    "    # Add user's message to the conversation history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    stream = False\n",
    "    try:\n",
    "        print('RETRIEVING FROM CACHE...')\n",
    "        print(chat_history)\n",
    "\n",
    "        tic = time.perf_counter()\n",
    "        response = complete(CACHECLIENT, chat_history, stream, TEMPERATURE)\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Retrieved cached response in {toc - tic:0.2f} seconds.\")\n",
    "        print(\"\\nCached response:\", response)\n",
    "\n",
    "        msg = displaycompletion(response, stream)\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": msg})\n",
    "\n",
    "    except openai.NotFoundError as e:\n",
    "        pass\n",
    "        print('REQUESTING API RESPONSE...')\n",
    "        print(chat_history)\n",
    "\n",
    "        stream = True\n",
    "        tic = time.perf_counter()\n",
    "        response = complete(OPENAICLIENT, chat_history, stream, TEMPERATURE)\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Retrieved API response in {toc - tic:0.2f} seconds.\")\n",
    "\n",
    "        msg = displaycompletion(response, stream)\n",
    "        print('Completion response:', msg)\n",
    "\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": msg})\n",
    "        updatecache(chat_history, TEMPERATURE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User A making repetitive requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global chat_history\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a customer support assistant for company ACME. Talk to a customer regarding his or her inquiries and concerns about ACME's products and services. ACME sells home electronics through their online store.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVING FROM CACHE...\n",
      "[{'role': 'system', 'content': \"You are a customer support assistant for company ACME. Talk to a customer regarding his or her inquiries and concerns about ACME's products and services. ACME sells home electronics through their online store.\"}, {'role': 'assistant', 'content': 'Hello, how can I help you today?'}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}]\n",
      "Retrieved cached response in 0.06 seconds.\n",
      "\n",
      "Cached response: ChatCompletion(id='chatcmpl-3wF7SbdzY92a4GjNQpJTCS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry to hear that you're looking for a refund. Can you please provide me with your order number and the reason for the refund so I can assist you further?\", role='assistant', function_call=None, tool_calls=None))], created=1710189788, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=0, prompt_tokens=0, total_tokens=0))\n",
      "I'm sorry to hear that you're looking for a refund. Can you please provide me with your order number and the reason for the refund so I can assist you further?\n"
     ]
    }
   ],
   "source": [
    "# First request\n",
    "message = \"I'd like to ask for a refund.\"\n",
    "await ask_canonical(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVING FROM CACHE...\n",
      "[{'role': 'system', 'content': \"You are a customer support assistant for company ACME. Talk to a customer regarding his or her inquiries and concerns about ACME's products and services. ACME sells home electronics through their online store.\"}, {'role': 'assistant', 'content': 'Hello, how can I help you today?'}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}, {'role': 'assistant', 'content': \"I'm sorry to hear that you're looking for a refund. Can you please provide me with your order number and the reason for the refund so I can assist you further?\"}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}]\n",
      "REQUESTING API RESPONSE...\n",
      "[{'role': 'system', 'content': \"You are a customer support assistant for company ACME. Talk to a customer regarding his or her inquiries and concerns about ACME's products and services. ACME sells home electronics through their online store.\"}, {'role': 'assistant', 'content': 'Hello, how can I help you today?'}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}, {'role': 'assistant', 'content': \"I'm sorry to hear that you're looking for a refund. Can you please provide me with your order number and the reason for the refund so I can assist you further?\"}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}]\n",
      "Retrieved API response in 0.44 seconds.\n",
      "Of course, I'd be happy to assist you with that. Can you please provide me with your order number and the reason for the refund so I can start the process for you?Completion response: Of course, I'd be happy to assist you with that. Can you please provide me with your order number and the reason for the refund so I can start the process for you?\n"
     ]
    }
   ],
   "source": [
    "# Identical request #2\n",
    "message = \"I'd like to ask for a refund.\"\n",
    "await ask_canonical(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVING FROM CACHE...\n",
      "[{'role': 'system', 'content': \"You are a customer support assistant for company ACME. Talk to a customer regarding his or her inquiries and concerns about ACME's products and services. ACME sells home electronics through their online store.\"}, {'role': 'assistant', 'content': 'Hello, how can I help you today?'}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}, {'role': 'assistant', 'content': \"I'm sorry to hear that you're looking for a refund. Can you please provide me with your order number and the reason for the refund so I can assist you further?\"}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}, {'role': 'assistant', 'content': \"Of course, I'd be happy to assist you with that. Can you please provide me with your order number and the reason for the refund so I can start the process for you?\"}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}]\n",
      "REQUESTING API RESPONSE...\n",
      "[{'role': 'system', 'content': \"You are a customer support assistant for company ACME. Talk to a customer regarding his or her inquiries and concerns about ACME's products and services. ACME sells home electronics through their online store.\"}, {'role': 'assistant', 'content': 'Hello, how can I help you today?'}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}, {'role': 'assistant', 'content': \"I'm sorry to hear that you're looking for a refund. Can you please provide me with your order number and the reason for the refund so I can assist you further?\"}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}, {'role': 'assistant', 'content': \"Of course, I'd be happy to assist you with that. Can you please provide me with your order number and the reason for the refund so I can start the process for you?\"}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}]\n",
      "Retrieved API response in 0.41 seconds.\n",
      "I understand that you're requesting a refund. To assist you further, could you please provide me with your order number and the reason for the refund? This information will help me process your request efficiently.Completion response: I understand that you're requesting a refund. To assist you further, could you please provide me with your order number and the reason for the refund? This information will help me process your request efficiently.\n"
     ]
    }
   ],
   "source": [
    "# Identical request #3\n",
    "message = \"I'd like to ask for a refund.\"\n",
    "await ask_canonical(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another User B making the same request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVING FROM CACHE...\n",
      "[{'role': 'system', 'content': \"You are a customer support assistant for company ACME. Talk to a customer regarding his or her inquiries and concerns about ACME's products and services. ACME sells home electronics through their online store.\"}, {'role': 'assistant', 'content': 'Hello, how can I help you today?'}, {'role': 'user', 'content': \"I'd like to ask for a refund.\"}]\n",
      "Retrieved cached response in 0.05 seconds.\n",
      "\n",
      "Cached response: ChatCompletion(id='chatcmpl-W7NLA3qFCvouSp6mvDbJhq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry to hear that you're looking for a refund. Can you please provide me with your order number and the reason for the refund so I can assist you further?\", role='assistant', function_call=None, tool_calls=None))], created=1710189815, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=0, prompt_tokens=0, total_tokens=0))\n",
      "I'm sorry to hear that you're looking for a refund. Can you please provide me with your order number and the reason for the refund so I can assist you further?\n"
     ]
    }
   ],
   "source": [
    "# Simulate another user asking the same request\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a customer support assistant for company ACME. Talk to a customer regarding his or her inquiries and concerns about ACME's products and services. ACME sells home electronics through their online store.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"}\n",
    "]\n",
    "message = \"I'd like to ask for a refund.\"\n",
    "await ask_canonical(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canonical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
