{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H9o0XGNw.RJL5iU8yuURKggXIsAHFNbzALCWFRNZw\n",
      "https://cacheapp.canonical.chat/\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "print(os.environ[\"CANONICAL_CACHE_API_KEY\"])\n",
    "print(os.environ[\"CANONICAL_CACHE_HOST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0\n",
    "CACHECLIENT = openai.OpenAI(\n",
    "    base_url=os.environ[\"CANONICAL_CACHE_HOST\"],\n",
    "    api_key=\"canbeanything\",\n",
    "    http_client=httpx.Client(\n",
    "        headers={\n",
    "            \"X-Canonical-Api-Key\": os.environ[\"CANONICAL_CACHE_API_KEY\"],\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "OPENAICLIENT = openai.OpenAI(\n",
    "    api_key= os.environ[\"OPENAI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(\n",
    "    client: openai.OpenAI,\n",
    "    messages: List[dict[str:str]],\n",
    "    stream: bool,\n",
    "    temperature: int = 0,\n",
    ") -> openai.ChatCompletion:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        stream=stream,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\n",
    "def displaycompletion(completion: openai.ChatCompletion, stream: bool) -> str:\n",
    "    msg = \"\"\n",
    "    if stream:\n",
    "        for chunk in completion:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                chunk = chunk.choices[0].delta.content\n",
    "                msg += chunk\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "    else:\n",
    "        msg = completion.choices[0].message.content\n",
    "        print(msg)\n",
    "    return msg\n",
    "\n",
    "\n",
    "def updatecache(messages: List[dict[str:str]], temp: int = 0) -> None:\n",
    "    requests.request(\n",
    "        method=\"POST\",\n",
    "        url=f\"{os.environ['CANONICAL_CACHE_HOST']}api/v1/cache\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Canonical-Api-Key\": os.environ[\"CANONICAL_CACHE_API_KEY\"],\n",
    "        },\n",
    "        data=json.dumps(\n",
    "            {\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": temp,\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "async def ask_canonical(message: str) -> None:\n",
    "    global chat_history\n",
    "    # Add user's message to the conversation history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    stream = False\n",
    "    try:\n",
    "\n",
    "        tic = time.perf_counter()\n",
    "        response = complete(CACHECLIENT, chat_history, stream, TEMPERATURE)\n",
    "        message_content = response.choices[0].message.content\n",
    "        toc = time.perf_counter()\n",
    "        print(\"\\nCache Hit\")\n",
    "        print(f\"\\nRetrieved cached response in {toc - tic:0.2f} seconds. I'm much faster outside of this dang Notebook! You should see me on-prem!\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        msg = displaycompletion(response, stream)\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": msg})\n",
    "\n",
    "    except openai.NotFoundError as e:\n",
    "        pass\n",
    "\n",
    "        stream = True\n",
    "        tic = time.perf_counter()\n",
    "        response = complete(OPENAICLIENT, chat_history, stream, TEMPERATURE)\n",
    "        toc = time.perf_counter()\n",
    "        print(\"\\nCache Miss\")\n",
    "        print(f\"\\nRetrieved API response in {toc - tic:0.2f} seconds.\")\n",
    "        print(\"\")                \n",
    "\n",
    "        msg = displaycompletion(response, stream)\n",
    "        \n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": msg})\n",
    "        updatecache(chat_history, TEMPERATURE)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"\\nCache writes are unavailable for 5 seconds to avoid conflicting with pre-fetching in Voice AI applications.\") \n",
    "        print(\"\\nEnforcing 5 second sleep...\")\n",
    "        time.sleep(5)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define The System Prompt and Cache Scope\n",
    "\n",
    "Each system prompt gets its own cache. To start a new cache, change the system prompt. The first time you use this demo, change the system prompt. Even a small change, like adding replacing one word, will create a new cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_CONTENT = (\"You are a customer support assistant for ACME 3-D wind sensorss company.\" + \n",
    "                         \"Talk to a customer regarding his or her inquiries and concerns about ACME's products and services.\" + \n",
    "                         \"ACME sells products through their online store.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User makes a request\n",
    "\n",
    "Here the user is making the first ever request on the new cache. The cache is currently empty so we expect a cache miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Miss\n",
      "\n",
      "Retrieved API response in 0.74 seconds.\n",
      "\n",
      "I'm sorry to hear that you're not satisfied with your purchase. Can you please provide me with your order number so I can look into your request for a refund?\n",
      "\n",
      "\n",
      "Cache writes are unavailable for 5 seconds to avoid conflicting with pre-fetching in Voice AI applications.\n",
      "\n",
      "Enforcing 5 second sleep...\n"
     ]
    }
   ],
   "source": [
    "global chat_history\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT_CONTENT},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"}\n",
    "]\n",
    "\n",
    "# First request\n",
    "message = \"I'd like to ask for a refund.\"\n",
    "await ask_canonical(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New user starts a new session with the same system prompt and cache. New user makes a similar request.\n",
    "\n",
    "Here a new user is chatting with the LLM. The first LLM response is the one and only response in the cache. If the user makes a similar request as the request that's in the cache, you'll get a cache hit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Hit\n",
      "\n",
      "Retrieved cached response in 0.80 seconds. I'm much faster outside of this dang Notebook! You should see me on-prem!\n",
      "\n",
      "\n",
      "I'm sorry to hear that you're not satisfied with your purchase. Can you please provide me with your order number so I can look into your request for a refund?\n"
     ]
    }
   ],
   "source": [
    "global chat_history\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT_CONTENT},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"}\n",
    "]\n",
    "\n",
    "message = \"I'd like a refund.\"\n",
    "await ask_canonical(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Turn\n",
    "\n",
    "Make a new request of the LLM that's similar to the previous requests. Do you get a cache hit?\n",
    "\n",
    "Make a request that's similar to the previous responses, but has a different intention. Do you get a cache miss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Hit\n",
      "\n",
      "Retrieved cached response in 0.53 seconds. I'm much faster outside of this dang Notebook! You should see me on-prem!\n",
      "\n",
      "\n",
      "I see that you have some inquiries or concerns about our products and services. Please feel free to share them with me so I can assist you accordingly.\n"
     ]
    }
   ],
   "source": [
    "global chat_history\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT_CONTENT},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"}\n",
    "]\n",
    "\n",
    "message = \"[write your message here.]\"\n",
    "await ask_canonical(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canonical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
